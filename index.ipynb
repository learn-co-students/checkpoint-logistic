{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["---\n", "## Introduction to Logistic Regression [Suggested Time: 25 min]\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!---\n", "# load data\n", "ads_df = pd.read_csv(\"raw_data/social_network_ads.csv\")\n", "\n", "# one hot encode categorical feature\n", "def is_female(x):\n", "    \"\"\"Returns 1 if Female; else 0\"\"\"\n", "    if x == \"Female\":\n", "        return 1\n", "    else:\n", "        return 0\n", "        \n", "ads_df[\"Female\"] = ads_df[\"Gender\"].apply(is_female)\n", "ads_df.drop([\"User ID\", \"Gender\"], axis=1, inplace=True)\n", "ads_df.head()\n", "\n", "# separate features and target\n", "X = ads_df.drop(\"Purchased\", axis=1)\n", "y = ads_df[\"Purchased\"]\n", "\n", "# train/test split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=19)\n", "\n", "# preprocessing\n", "scale = StandardScaler()\n", "scale.fit(X_train)\n", "X_train = scale.transform(X_train)\n", "X_test = scale.transform(X_test)\n", "\n", "# save preprocessed train/test split objects\n", "pickle.dump(X_train, open(\"write_data/social_network_ads/X_train_scaled.pkl\", \"wb\"))\n", "pickle.dump(X_test, open(\"write_data/social_network_ads/X_test_scaled.pkl\", \"wb\"))\n", "pickle.dump(y_train, open(\"write_data/social_network_ads/y_train.pkl\", \"wb\"))\n", "pickle.dump(y_test, open(\"write_data/social_network_ads/y_test.pkl\", \"wb\"))\n", "\n", "# build model\n", "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n", "model.fit(X_train, y_train)\n", "y_test_pred = model.predict(X_test)\n", "y_train_pred = model.predict(X_train)\n", "\n", "from sklearn.metrics import confusion_matrix\n", "\n", "# create confusion matrix\n", "# tn, fp, fn, tp\n", "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n", "cnf_matrix\n", "\n", "# build confusion matrix plot\n", "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n", "\n", "# Add title and Axis Labels\n", "plt.title('Confusion Matrix')\n", "plt.ylabel('True label')\n", "plt.xlabel('Predicted label')\n", "\n", "# Add appropriate Axis Scales\n", "class_names = set(y_test) #Get class labels to add to matrix\n", "tick_marks = np.arange(len(class_names))\n", "plt.xticks(tick_marks, class_names)\n", "plt.yticks(tick_marks, class_names)\n", "\n", "# Add Labels to Each Cell\n", "thresh = cnf_matrix.max() / 2. #Used for text coloring below\n", "#Here we iterate through the confusion matrix and append labels to our visualization.\n", "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n", "        plt.text(j, i, cnf_matrix[i, j],\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n", "\n", "# Add a Side Bar Legend Showing Colors\n", "plt.colorbar()\n", "\n", "# Add padding\n", "plt.tight_layout()\n", "plt.savefig(\"visuals/cnf_matrix.png\",\n", "            dpi=150,\n", "            bbox_inches=\"tight\")\n", "--->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![cnf matrix](visuals/cnf_matrix.png)\n", "\n", "### 1. Using the confusion matrix up above, calculate precision, recall, and F-1 score."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["precision: 0.8823529411764706\n", "recall: 0.7142857142857143\n", "F1: 0.7894736842105262\n"]}], "source": ["precision = 30/(30+4)\n", "recall = 30 / (30 + 12)\n", "F1 = 2 * (precision * recall) / (precision + recall)\n", "\n", "print(\"precision: {}\".format(precision))\n", "print(\"recall: {}\".format(recall))\n", "print(\"F1: {}\".format(F1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.  What is a real life example of when you would care more about recall than precision? Make sure to include information about errors in your explanation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# We would care more about recall than precision in cases where a Type II error (a False Negative) would \n", "# have serious consequences. An example of this would be a medical test that determines if someone has a serious disease.\n", "# A higher recall would mean that we would have a higher chance of identifying all people who ACTUALLY had the serious disease."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!---\n", "# save preprocessed train/test split objects\n", "X_train = pickle.load(open(\"write_data/social_network_ads/X_train_scaled.pkl\", \"rb\"))\n", "X_test = pickle.load(open(\"write_data/social_network_ads/X_test_scaled.pkl\", \"rb\"))\n", "y_train = pickle.load(open(\"write_data/social_network_ads/y_train.pkl\", \"rb\"))\n", "y_test = pickle.load(open(\"write_data/social_network_ads/y_test.pkl\", \"rb\"))\n", "\n", "# build model\n", "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n", "model.fit(X_train, y_train)\n", "y_test_pred = model.predict(X_test)\n", "y_train_pred = model.predict(X_train)\n", "\n", "labels = [\"Age\", \"Estimated Salary\", \"Female\", \"All Features\"]\n", "colors = sns.color_palette(\"Set2\")\n", "plt.figure(figsize=(10, 8))\n", "# add one ROC curve per feature\n", "for feature in range(3):\n", "    # female feature is one hot encoded so it produces an ROC point rather than a curve\n", "    # for this reason, female will not be included in the plot at all since it is\n", "    # disingeneuous to call it a curve.\n", "    if feature == 2:\n", "        pass\n", "    else:\n", "        X_train_feat = X_train[:, feature].reshape(-1, 1)\n", "        X_test_feat = X_test[:, feature].reshape(-1, 1)\n", "        logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='lbfgs')\n", "        model_log = logreg.fit(X_train_feat, y_train)\n", "        y_score = model_log.decision_function(X_test_feat)\n", "        fpr, tpr, thresholds = roc_curve(y_test, y_score)\n", "        lw = 2\n", "        plt.plot(fpr, tpr, color=colors[feature],\n", "                 lw=lw, label=labels[feature])\n", "\n", "# add one ROC curve with all the features\n", "model_log = logreg.fit(X_train, y_train)\n", "y_score = model_log.decision_function(X_test)\n", "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n", "lw = 2\n", "plt.plot(fpr, tpr, color=colors[3], lw=lw, label=labels[3])\n", "\n", "# create foundation of the plot\n", "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n", "plt.xlim([0.0, 1.0])\n", "plt.ylim([0.0, 1.05])\n", "plt.yticks([i / 20.0 for i in range(21)])\n", "plt.xticks([i / 20.0 for i in range(21)])\n", "plt.xlabel(\"False positive rate\")\n", "plt.ylabel(\"True positive rate\")\n", "plt.title(\"ROC Curve\")\n", "plt.legend()\n", "plt.tight_layout()\n", "plt.savefig(\"visuals/many_roc.png\",\n", "            dpi=150,\n", "            bbox_inches=\"tight\")\n", "--->"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3. Pick the best ROC curve from this graph and explain your choice. \n", "\n", "*Note: each ROC curve represents one model, each labeled with the feature(s) inside each model*.\n", "\n", "<img src = \"visuals/many_roc.png\" width = \"700\">\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# The best ROC curve in this graph is for the one that contains all features (the pink one). \n", "# This is because it has the largest area under the curve. The ROC curve is created by obtaining\n", "# the ratio of the True Positive Rate to the False Positive Rate over all thresholds of a classification model."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import itertools\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns\n", "import numpy as np\n", "from sklearn.linear_model import Lasso, Ridge\n", "import pickle\n", "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!---\n", "# sorting by 'Purchased' and then dropping the last 130 records\n", "dropped_df = ads_df.sort_values(by=\"Purchased\")[:-130]\n", "dropped_df.reset_index(inplace=True)\n", "pickle.dump(dropped_df, open(\"write_data/sample_network_data.pkl\", \"wb\"))\n", "--->"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The original classifier has an accuracy score of 0.956.\n", "The original classifier has an area under the ROC curve of 0.836.\n"]}], "source": ["network_df = pickle.load(open(\"write_data/sample_network_data.pkl\", \"rb\"))\n", "\n", "# partion features and target \n", "X = network_df.drop(\"Purchased\", axis=1)\n", "y = network_df[\"Purchased\"]\n", "\n", "# train test split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n", "\n", "# scale features\n", "scale = StandardScaler()\n", "scale.fit(X_train)\n", "X_train = scale.transform(X_train)\n", "X_test = scale.transform(X_test)\n", "\n", "# build classifier\n", "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n", "model.fit(X_train,y_train)\n", "y_test_pred = model.predict(X_test)\n", "\n", "# get the accuracy score\n", "print(f\"The original classifier has an accuracy score of {round(accuracy_score(y_test, y_test_pred), 3)}.\")\n", "\n", "# get the area under the curve from an ROC curve\n", "y_score = model.decision_function(X_test)\n", "fpr, tpr, _ = roc_curve(y_test, y_score)\n", "auc = round(roc_auc_score(y_test, y_score), 3)\n", "print(f\"The original classifier has an area under the ROC curve of {auc}.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4. The model above has an accuracy score that might be too good to believe. Using `y.value_counts()`, explain how `y` is affecting the accuracy score."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": ["0    257\n", "1     13\n", "Name: Purchased, dtype: int64"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["y.value_counts()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "# y.value_counts() indicates that we have a class imbalance. When we have class imbalance our model only learns to\n", "# predict one class and is not penalized for doing so, because it is still getting the right answer most of the time. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5. What methods would you use to address the issues mentioned up above in question 4? \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "#Any one of these is an acceptable answer : \n", "\n", "# Class imbalance could be rectified using SMOTE to generate additional synthetic data points for the minority class so\n", "# that we have equal (or almost equal) number of data points in each class. \n", "\n", "# Class imbalance could be rectified using oversampling to sample (with replacement) from the minority class until we\n", "# have equal samples from both classes. "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.9"}}, "nbformat": 4, "nbformat_minor": 2}